name: RAGAS
description: A framework for performing Retrieval-Augmented Generation (RAG)
  evaluation, supporting multiple ways of validating results.
source_url: https://docs.ragas.io/en/latest/getstarted/
featured: false
category: llm-tools
tags:
  - rag
  - evaluation
  - llm
markdown: >-
  **Description**

  Ragas is a framework designed for evaluating Retrieval-Augmented Generation
  (RAG) systems, supporting multiple ways of validating results.


  **Features**

  *   **LLM Application Evaluation**: Enables evaluation of your first LLM
  application.

  *   **RAG System Evaluation**: Provides capabilities to evaluate simple RAG
  setups.

  *   **Synthetic Testset Generation**: Allows generation of synthetic test
  datasets specifically for RAG systems.

  *   **Comprehensive Evaluation Metrics**: Offers a wide array of metrics for
  assessing RAG and LLM performance, categorized as:
      *   **Retrieval Augmented Generation Metrics**:
          *   Context Precision
          *   Context Recall
          *   Context Entities Recall
          *   Noise Sensitivity
          *   Response Relevancy
          *   Faithfulness
      *   **Nvidia Metrics**:
          *   Answer Accuracy
          *   Context Relevance
          *   Response Groundedness
      *   **Agents or Tool Use Cases**
  *   **Core Components**: Utilizes key components such as Prompts, Evaluation
  Samples, and Evaluation Datasets to facilitate thorough assessments.
updated_at: 2025-07-01 00:28
